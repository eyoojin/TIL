{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36319236-5992-4dc0-9d9d-341e12f6e61a",
   "metadata": {},
   "source": [
    "# 01. 나의 첫 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d747b12-b12a-4cb4-8f4f-59313666d772",
   "metadata": {},
   "source": [
    "## 1-1. 인공지능과 머신러닝, 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d86c8-5fab-4ce2-b2b7-188c70d4661a",
   "metadata": {},
   "source": [
    "- `인공지능`: 사람처럼 학습하고 추론할 수 있는 지능을 가진 시스템을 만드는 기술\n",
    "    - **강인공지능**과 **약인공지능**으로 나뉨\n",
    "- `머신러닝`: 규칙을 프로그래밍하지 않아도 자동으로 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야\n",
    "    - 대표적인 라이브러리: **사이킷런**\n",
    "- `딥러닝`: **인공 신경망**을 기반으로 한 방법\n",
    "    - 대표적인 라이브러리: **텐서플로**, **파이토치**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539a2d6-ba10-48da-8c3c-cc8815b5bd8c",
   "metadata": {},
   "source": [
    "## 1-2. 코랩과 주피터 노트북"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a92d2a-2d89-4c80-9c93-b908ce627661",
   "metadata": {},
   "source": [
    "1. 구글에서 제공하는 웹 브라우저 기반의 파이썬 실행 환경: `코랩`\n",
    "2. 코랩 노트북에서 쓸 수 있는 마크다운 중에서 기울임 꼴로 쓰는 것: `_혼공머신_`\n",
    "3. 코랩 노트북이 실행되는 곳: `구글 클라우드`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b2311-5503-46cc-adab-322a9cdaebec",
   "metadata": {},
   "source": [
    "## 1-3. 마켓과 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84c89b-d63f-4e6c-bcf7-b0c1157f64f7",
   "metadata": {},
   "source": [
    "- `특성`: 데이터를 표현하는 하나의 성질\n",
    "    - 예시: 생선 데이터의 길이와 무게\n",
    "- `훈련`: 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정\n",
    "    - 사이킷런의 **fit()**\n",
    "- `k-최근접 이웃 알고리즘`: 어떤 데이터에 대한 답을 구할 때 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 사용\n",
    "    - 어떤 규칙을 찾기보다는 전체 데이터를 메모리에 가지고 있는 것이 전부임\n",
    "- `모델`: 머신러닝 프로그램에서 알고리즘이 구현된 객체\n",
    "    - 종종 알고리즘 자체를 모델이라고 부르기도 함\n",
    "- `정확도`: 정확한 답을 몇 개 맞혔는지를 백분율로 나타낸 값\n",
    "    - 사이킷런에서는 0~1 사이의 값으로 출력됨\n",
    "    - 정확도 = (정확히 맞힌 개수) / (전체 데이터 개수)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f0e08-2e12-4ca4-9f1f-d72f6e5a6c21",
   "metadata": {},
   "source": [
    "1. 데이터를 표현하는 하나의 성질로써, 예를 들어 국가 데이터의 경우 인구 수, GDP, 면적 등이 하나의 국가를 나타내는데, 머신러신에서 부르는 이런 성질: `특성`\n",
    "2. 가장 가까운 이웃을 참고하여 정답을 예측하는 알고리즘이 구현된 사이킷런의 클래스: `KNeighborsClassifier`\n",
    "3. 사이킷런 모델을 훈련할 때 사용하는 메서드: `fit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cbf0bd-1479-4276-8cd1-a0dbe4a30355",
   "metadata": {},
   "source": [
    "# 02. 데이터 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44dfa10-7d28-4f7e-a25a-52ccaeb27218",
   "metadata": {},
   "source": [
    "## 2-1. 훈련 세트와 테스트 세트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a9f474-1070-4942-8aab-de8467bdd86c",
   "metadata": {},
   "source": [
    "- `지도 학습`: 입력과 타깃을 전달하여 모델을 훈련한 다음 새로운 데이터를 예측하는 데 활용\n",
    "    - 예시: **k-최근접 이웃**\n",
    "- `비지도 학습`: 타깃 데이터가 없어서 무엇을 예측하는 것이 아니라 입력 데이터에서 어떤 특징을 찾는 데 활용\n",
    "- `훈련 세트`: 모델을 훈련할 때 사용하는 데이터\n",
    "    - 보통 훈련 세트가 클수록 좋기 때문에 테스트 세트를 제외한 모든 데이터를 사용\n",
    "- `테스트 세트`: 평가에 사용하는 데이터\n",
    "    - 전체 데이터에서 20~30%를 사용\n",
    "    - 전체 데이터가 아주 크다면 1%만 덜어내도 충분할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bdff49-d95e-4780-9be9-56e3c9dc7743",
   "metadata": {},
   "source": [
    "1. 머신러닝 알고리즘의 한 종류로서 샘플의 입력과 타깃(정답)을 알고 있을 때 사용할 수 있는 학습 방법: `지도 학습`\n",
    "2. 훈련 세트와 테스트 세트가 잘못 만들어져 데이터를 대표하지 못하는 현상: `샘플링 편향`\n",
    "3. 사이킷런이 기대하는 입력 데이터(배열) 구성: `행: 샘플, 열: 특성`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c17e2-f868-462b-bf48-c3f42108d19b",
   "metadata": {},
   "source": [
    "## 2-2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bda6b-9d1d-456d-a0d9-f08d9d9d1251",
   "metadata": {},
   "source": [
    "- `데이터 전처리`: 머신러닝 모델에 훈련 데이터를 주입하기 전에 가공하는 단계\n",
    "- `표준점수`: 훈련 세트의 스케일을 바꾸는 대표적인 방법 중 하나\n",
    "    - 특성의 평균을 빼고 표준편차로 나눔\n",
    "    - 훈련세트의 평균과 표준편차로 테스트 세트를 바꿔야 함\n",
    "- `브로드캐스팅`: 크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이나 열로 확장하여 수행하는 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c691b359-1703-4ea3-ad46-573cc1c9d049",
   "metadata": {},
   "source": [
    "1. 스케일 조정 방식의 하나로 특성값을 평균에서 표준편차의 몇 배수만큼 떨어져 잇는지로 변환한 값: `표준점수`\n",
    "2. 테스트 세트의 스케일을 조정하려고 할 때 사용해야하는 데이터의 통계 값: `훈련 세트`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ede1b-af8a-4e84-b033-8988640867ec",
   "metadata": {},
   "source": [
    "# 03. 회귀 알고리즘과 모델 규제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e657eb5-c56b-4799-a3b3-d92b060eaae6",
   "metadata": {},
   "source": [
    "## 3-1. k-최근접 이웃 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e90a714-f161-4ff7-bbe8-5b408ce5895f",
   "metadata": {},
   "source": [
    "- `회귀`: 임이의 수치를 예측하는 문제\n",
    "    - 타깃값도 임의의 수치가 됨\n",
    "- `k-최근접 이웃 회귀`: k-최근접 이웃 알고리즘을 사용해 회귀 문제를 푸는 것\n",
    "    - 가장 가까운 이웃 샘플을 찾고 이 샘플들의 타깃값을 평균하여 예측으로 삼음\n",
    "- `결정계수(R**2)`: 대표적인 회귀 문제의 성능 측정 도구\n",
    "    - 1에 가까울수록 좋고, 0에 가깝다면 성능이 나쁜 모델\n",
    "- `과대적합`: 모델의 훈련 세트 성능이 테스트 세트 성능보다 훨씬 높을 때\n",
    "    - 모델이 훈련 세트에 너무 집착해서 데이터에 내재된 거시적인 패턴을 감지하지 못함\n",
    "- `과소적합`: 훈련 세트와 테스트 세트 성능이 모두 동일하게 낮거나 테스트 세트 성능이 오히려 더 높을 때\n",
    "    - 더 복잡한 모델을 사용해 훈련 세트에 잘 맞는 모델을 만들어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00604a7a-5526-4013-ac79-22e2e8107b47",
   "metadata": {},
   "source": [
    "1. k-최근접 이웃 회귀에서 새로운 샘플에 대한 예측을 하는 법: `이웃 샘플의 타깃값의 평균`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0166dc-af09-430f-ac6d-6040034dc1b2",
   "metadata": {},
   "source": [
    "## 3-2. 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2b8dd-104f-4c17-876d-72ea22ade1f9",
   "metadata": {},
   "source": [
    "- `선형 회귀`: 특성과 타깃 사이의 관계를 가장 잘 나타내는 선형 방정식을 찾음\n",
    "    - 특성이 하나일 경우 직선 방정식이 됨\n",
    "- 선형 방정식의 `계수` 또는 `가중치`: 선형 회귀가 찾은 특성과 타깃 사이의 관계가 저장됨\n",
    "    - 머신러닝에서 종종 가중치는 방정식의 기울기와 절편을 모두 의미하는 경우가 많음\n",
    "- `모델 파라미터`: 선형 회귀가 찾은 가중치처럼 머신러닝 모델이 특성에서 학습한 파라미터\n",
    "- `다항 회귀`: 다항식을 사용하여 특성과 타깃 사이의 관계를 나타냄\n",
    "    - 이 함수는 비선형일 수 있지만 여전히 선형 회귀로 표현 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a063dd-5376-4f15-8dee-ed23365db481",
   "metadata": {},
   "source": [
    "1. 선형 회귀 모델이 찾은 방정식의 계수: `모델 파라미터`\n",
    "2. 사이킷런에서 다항 회귀 모델을 훈련할 수 있는 클래스: `LinearRegression`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc835ab-8421-4906-8af3-dc02d92d878f",
   "metadata": {},
   "source": [
    "## 3-3. 특성 공학과 규제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b39cdd-889a-4fec-a86f-3230901e37f3",
   "metadata": {},
   "source": [
    "- `다중 회귀`: 여러개의 특성을 사용하는 회귀 모델\n",
    "    - 특성이 많으면 선형 모델은 강력한 성능을 발휘함\n",
    "- `특성 공학`: 주어진 특성을 조합하여 새로운 특성을 만드는 일련의 작업 과정\n",
    "- `릿지`: 규제가 있는 선형 회귀 모델\n",
    "    - 선형 모델의 계수를 작게 만들어 과대적합을 완화\n",
    "- `라쏘`: 규제가 있는 선형 회귀 모델\n",
    "    - 릿지와 달리 계수 값을 아예 0으로 만들 수도 있음\n",
    "- `하이퍼파라미터`: 머신러닝 알고리즘이 학습하지 않는 파라미터\n",
    "    - 사람이 사정에 지정해야 함\n",
    "    - 예시: 릿지와 라쏘의 규제 강도 alpha 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2299e1-79fa-48a0-821b-c509b84929cf",
   "metadata": {},
   "source": [
    "1. a, b, c 특성으로 이루어진 훈련 세트를 PolynomialFeatures(degree=3)으로 변환했을 때 변환된 데이터에 포함되는 특성: `1, a, a*b`\n",
    "2. 특성을 표준화하는 사이킷런 변환기 클래스: `StandardScaler`\n",
    "3. 과대적합과 과소적합\n",
    "    - 과대적합인 모델은 훈련 세트의 점수가 높음\n",
    "    - 과소적합인 모델은 훈련세트와 테스트 세트의 점수가 낮음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02d9af-c278-4352-8d8c-2071c564a1a7",
   "metadata": {},
   "source": [
    "# 04. 다양한 분류 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6320784-04be-43aa-8083-a9faef32657a",
   "metadata": {},
   "source": [
    "## 4-1. 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5021a-ce9c-4568-bb6b-5b177f0ec206",
   "metadata": {},
   "source": [
    "- `로지스틱 회귀`: 선형 방정식을 사용한 분류 알고리즘\n",
    "    - 선형 회귀와 달리 **시그모이드 함수**나 **소프트맥스 함수**를 사용하여 클래스 확률을 출력할 수 있음\n",
    "- `다중 분류`: 타깃 클래스가 2개 이상인 분류 문제\n",
    "    - 로지스틱 회귀는 다중 분류를 위해 소프트맥스 함수를 사용하여 클래스를 예측\n",
    "- `시그모이드 함수`: 선형 방정식의 출력을 0과 1 사이의 값으로 압축\n",
    "    - 이진 분류를 위해 사용\n",
    "- `소프트맥스 함수`: 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd49652-2fab-4faf-b897-6e02d678de29",
   "metadata": {},
   "source": [
    "1. 2개보다 많은 클래스가 있는 분류 문제: `다중 분류`\n",
    "2. 로지스틱 회귀가 이진 분류에서 확률을 출력하기 위해 사용하는 함수: `시그모이드 함수`\n",
    "3. decision_function() 메서드의 출력이 0일 때 시그모이드 함수의 값: `1 / (1 + e^(-0)) = 1 / (1 + 1) = 0.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00dcd23-196a-47e8-b9cb-21d8dcc42ab9",
   "metadata": {},
   "source": [
    "## 4-2. 확률적 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e181bda-c8fc-46a4-86b0-d275fe7aa76c",
   "metadata": {},
   "source": [
    "- `확률적 경사 하강법`: 훈련 세트에서 샘플 하나씩 꺼내 **손실 함수**의 경사를 따라 최적의 모델을 찾는 알고리즘\n",
    "    - 샘플을 하나씩 사용하지 않고 여러개를 사용하면 미니배치 경사 하강법\n",
    "    - 한번에 전체 샘플을 사용하면 배치 경사 하강법\n",
    "- `손실 함수`: 확률적 경사 하강법이 최적화할 대상\n",
    "    - 대부분의 문제에 잘 맞는 손실 함수가 이미 정의되어 있음\n",
    "    - 이진 분류: 로지스틱 회귀 또는 이진 크로스엔트로피 손실 함수\n",
    "    - 다중 분류: 크로스엔트로피 손실 함수\n",
    "    - 회귀 문제: 평균 제곱 오차 손실 함수\n",
    "- `에포크`: 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번 반복을 의미\n",
    "    - 일반적으로 경사 하강법 알고리즘은 수십에서 수백 번의 에포크를 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932538ae-d706-4e36-a40d-261d74a74f9d",
   "metadata": {},
   "source": [
    "1. 표준화 같은 데이터 전처리를 수행하지 않아도 되는 방식으로 구현된 클래스: `LinearRegression`\n",
    "2. 경사 하강법 알고리즘의 하나로 훈련 세트에서 몇개의 샘플을 뽑아서 훈련하는 방식: `미니배치 경사 하강법`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce2269-2f5d-477e-9e90-14a1cfd73359",
   "metadata": {},
   "source": [
    "# 07. 딥러닝을 시작합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc06b9-8b17-4ef8-b0bb-29767bb7754c",
   "metadata": {},
   "source": [
    "## 7-1. 인공 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd7b08-7d3f-4234-be20-15c61a47ee2a",
   "metadata": {},
   "source": [
    "- `인공 신경망`: 생물학적 뉴런에서 영감을 받아 만든 머신러닝 알고리즘\n",
    "    - 이름이 신경망이지만 실제 우리 뇌를 모델링한 것은 아님\n",
    "    - 신경망은 기존의 머신러닝 알고리즘으로 다루기 어려웠던 이미지, 음성, 텍스트 분야에서 뛰어난 성능을 발휘\n",
    "    - 종종 딥러닝이라고도 부름\n",
    "- `텐서플로`: 구글이 만든 딥러닝 라이브러리\n",
    "    - CPU와 GPU를 사용해 인공 신경망 모델을 효율적으로 훈련하며 모델 구축과 서비스에 필요한 다양한 도구를 제공\n",
    "    - 텐스플로 2.0부터는 신경망 모델을 빠르게 구성할 수 있는 케라스를 핵심 API로 채택\n",
    "    - 케라스를 사용하면 간단한 모델에서 아주 복잡한 모델까지 손쉽게 만들 수 있음\n",
    "- `밀집층`: 가장 간단한 인공 신경망의 층(인공 신경망에는 여러종류의 층이 있음)\n",
    "    - 뉴런들이 모두 연결되어 있기 때문에 완전 연결 층이라고도 부름\n",
    "    - 특별히 출력층에 밀집층을 사용할 때는 분류하려는 클래스오 동일한 개수의 뉴런을 사용\n",
    "- `원-핫 인코딩`: 정수값 배열에서 해당 정수 위치의 원소만 1이고 나머지는 모두 0으로 변환\n",
    "    - 다중 분류에서 출력층에서 만든 확률과 크로스 엔트로피 손실을 계산하기 위해 사용\n",
    "    - 텐서플로에서는 **sparse_categorical_entropy** 손실을 지정하면 이런 변환을 수행할 필요가 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1fcc43-62ea-4df1-9a3d-0389206db483",
   "metadata": {},
   "source": [
    "1. 어떤 인공 신경망의 입력 특성이 100개이고 밀집층에 있는 뉴런 개수가 10개일 때 필요한 모델 파라미터의 개수: `1010개`\n",
    "2. 케라스의 Dense 클래스를 사용해 신경망의 출력층을 만들려고 할 때 이 신경망이 이진 분류 모델이라면 activation 매개변수에 지정해야하는 활성화 함수: `sigmoid`\n",
    "    - softmax: 다중 분류 신경망의 출력층에 사용\n",
    "    - relu: 이미지를 다루는 문제에서 자주 사용\n",
    "3. 케라스 모델에서 손실 함수와 측정 지표 등을지정하는 메서드: `compile()`\n",
    "    - loss 매개변수로 손실 함수를 지정\n",
    "    - metrics 매개변수에서 측정하려는 지표를 지정\n",
    "4. 정수 레이블을 타깃으로 가지는 다중 분류 문제일 때 케라스 모델의 compile() 메서드에 지정할 손실 함수: `sparse_categotical_crossentropy`\n",
    "    - categorical_crossentropy: 타깃값이 원-핫 인코딩된 경우 사용\n",
    "    - binary_crossentropy: 이진 분류에 사용하는 손실 함수\n",
    "    - mean_square_error: 회귀 문제에 사용하는 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff01041-45cf-4121-bf6c-f40b58f97541",
   "metadata": {},
   "source": [
    "## 7-2. 심층 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f01a8e-7294-4586-b5f6-670312b5be0a",
   "metadata": {},
   "source": [
    "- `심층 신경망`: 2개 이상의 층을 포함한 신경망\n",
    "    - 다층 인공신경망, 심층 신경망, 딥러닝을 같은 의미로 사용하기도 함\n",
    "- `렐루 함수`: 이미지 분류 모델의 은닉층에 많이 사용하는 활성화 함수\n",
    "    - 시그모이드 함수는 층이 많을수록 활성화 함수의 양쪽 끝에서 변화가 작기 때문에 학습이 어려워지지만 렐루 함수는 이런 문제가 없으며 계산이 간단\n",
    "- `옵티마이저`: 신경망의 가중치와 절편을 학습하기 위한 알고리즘 또는 방법\n",
    "    - 케라스에는 다양한 경사 하강법 알고리즘이 구현되어 있음\n",
    "    - 예시: SGD, 네스테로프 모멘텀, RMSprop, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86579829-1786-46aa-944f-fddd5cbb9665",
   "metadata": {},
   "source": [
    "1. 모델의 add() 메서드 사용법: `model.add(keras.layers.Dense(10, activation='relu'))`\n",
    "2. 크기가 300 X 300인 입력을 케라스 층으로 펼치려고 할 때 사용해야하는 층: `Flatten`\n",
    "3. 이미지 분류를 위한 심층 신경망에 널리 사용되는 케라스의 활성화 함수: `relu`\n",
    "4. 적응적 학습률을 사용하지 않는 옵티마이저: `SGD`\n",
    "    - 기본 경사 하강법과 모멘텀, 네스테로프 모멘텀 알고리즘을 구현할 클래스\n",
    "    - 적응적 학습률 옵티마이저: Adagrad, RMSprop, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd27c1-86c2-449d-b250-27c16c453f13",
   "metadata": {},
   "source": [
    "## 7-3. 신경망 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44513e-cd60-43d1-830d-86ffbb350437",
   "metadata": {},
   "source": [
    "- `드롭아웃`: 은닉층에 있는 뉴런의 출력을 랜덤하게 꺼서 과대적합을 막는 기법\n",
    "    - 훈련 중에 적용되며 평가나 예측에서는 사용하지 않음\n",
    "    - 텐서플로는 이를 자동으로 처리\n",
    "- `콜백`: 케라스 모델을 훈련하는 도중에 어떤 작업을 수행할 수 있도록 도와주는 도구\n",
    "    - 최상의 모델을 자동으로 저장해 주거나 검증 점수가 더 이상 향상되지 않으면 일찍 종료\n",
    "- `조기 종료`: 검증 점수가 더 이상 감소하지 않고 상승하여 과대적합이 일어나면 훈련을 계속 진행하지 않고 멈추는 기법\n",
    "    - 계산 비용과 시간 절약 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ff0e2-3d2e-4a8f-b618-78d15ef599fc",
   "metadata": {},
   "source": [
    "1. 케라스 모델의 fit() 메서드에 검증 세트를 전달하는 코드: `model.fit(... validation_data=(val_input, val_target))`\n",
    "2. 이전 층의 뉴런 출력 중 70%만 사용하기 위해 드롭아웃 층을 추가하는 법: `Dropout(0.3)`\n",
    "3. 케라스 모델의 가중치만 저장하는 메서드: `save_weights()`\n",
    "4. 케라스의 조기 종료 콜백을 사용하려고 할 때 3번의 에포크 동안 손실이 감소되지 않으면 종료하고 최상의 모델 가중치를 복원하도록 설정: `EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
