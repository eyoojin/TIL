# 1 검색 시스템 이해하기
## 1.1 검색 시스템의 이해
- 검색엔진과 데이터베이스 비교
- 검색 시스템의 등장 배경, 검색 시스템의 특징 및 필요성

### 1.1.1 검색 시스템이란?
- 검색 엔진(search engine)
    - 광활한 웹에서 정보를 수집해 검색 결과를 제공하는 프로그램
    - 검색 결과로 제공되는 데이터의 특성에 따라 구현 형태가 달라짐
    - 야후: 디렉터리 기반의 검색 결과를 세계 최초로 제공
    - 요즘에는 뉴스, 블로그, 카페 등 대범주에 따른 카테고리별 검색 결과를 대부분의 검색 업체에서 제공하고 있음

- 검색 시스템(search system)
    - 대용량 데이터를 기반으로 신뢰성 있는 검색 결과를 제공하기 위해 검색엔진을 기반으로 구축된 시스템을 통칭
    - 수집기를 이용해 방대한 데이터를 수집하고 이를 다수의 검색엔진을 이용해 색인하고 검색 결과를 UI로 제공
    - 시스템 내부의 정책에 따라 관련도가 높은 문서를 검색 결과의 상위에 배치할 수 있음
    - 특정 필드나 문서에 가중치를 둬서 검색의 정확도를 높일 수 있음

- 검색 서비스(search service)
    - 검색엔진을 기반으로 구축한 검색 시스템을 활용해 검색 결과를 서비스로 제공

- `검색 서비스 > 검색 시스템 > 검색엔진`

### 1.1.2 검색 시스템의 구성 요소
- **수집기**
    - 웹사이트, 블로그, 카페 등 웹에서 필요한 정보를 수집하는 프로그램
    - 크롤러, 스파이더, 웜, 웹 로봇 등으로도 불림
    - 파일, 데이터베이스, 웹페이지 등 웹상의 대부분의 정보가 수집 대상
    - 파일의 경우 수집기가 파일명, 파일 내용, 파일 경로 등의 정보를 수집하고 저장하면 검색엔진이 저장된 정보를 검색하고 사용자 질의에 답함

- **스토리지**
    - 데이터베이스에서 데이터를 저장하는 물리적인 저장소
    - 검색엔진은 색인한 데이터를 스토리지에 보관함

- **색인기**
    - 검색엔진이 수집한 정보에서 사용자 질의와 일치하는 정보를 찾으려면 수집된 데이터를 검색 가능한 구조로 가공하고 저장해야 함 => 그 역할을 하는 것이 색인기
    - 다양한 형태소 분석기를 조합해 정보에서 의미가 있는 용어를 추출하고 검색에 유리한 역색인 구조로 데이터를 저장함

- **검색기**
    - 사용자 질의를 입력받아 색인기에서 저장한 역색인 구조에서 일치하는 문서를 찾아 결과로 반환
    - 질의와 문서가 일치하는지는 유사도 기반의 검색 순위 알고리즘으로 판단
    - 색인기와 마찬가지로 형태소 분석기를 이용해 사용자 질의에서 유의미한 용어를 추출해 검색
    - 사용하는 형태소 분석기에 따라 검색 품질이 달라짐

- 검색 시스템의 구성
    - 수집기 => 색인기 => (스토리지) => 검색기

### 1.1.3 관계형 데이터베이스와의 차이점
- 검색엔진과 관계형 데이터베이스(RDBMS) 모두 질의와 일치하는 데이터를 찾아 사용자에게 제공한다는 점에서 유사점이 많음 but 관계형 데이터베이스로 검색 기능을 제공하는 데는 문제점이 존재

- 데이터베이스
    - 데이터를 통합 관리하는 데이터의 집합
    - 저장 방식에 따라 크게 관계형 또는 계층형 데이터베이스로 나뉨
    - 모든 데이터는 중복을 제거하고 정형 데이터로 구조화해 행과 열로 구성된 데이블에 저장됨
    - SQL 문을 이용해 원하는 정보의 검색이 가능한데 텍스트 매칭을 통한 단순한 검색만 가능
    - 텍스트를 여러 단어로 변형하거나 여러 개의 동의어나 유의어를 활용한 검색은 불가능함

- 검색엔진
    - 데이터베이스에서는 불가능한 비정형 데이터를 색인하고 검색할 수 있음
    - 형태소 분석을 통해 사람이 구사하는 자연어의 처리가 가능해지고 역색인 구조를 바탕으로 빠른 검색 속도를 보장

- 엘라스틱서치와 관계형 데이터베이스 비교

|엘라스틱서치|관계형 데이터베이스|
|---|---|
|인덱스|데이터베이스|
|샤드|파티션|
|타입|테이블|
|문서|행|
|필드|열|
|매핑|스키마|
|Query DSL|SQL|

- 엘라스틱서치의 인덱스: 관계형 데이터베이스의 데이터베이스와 비슷한 문서의 모음
- 엘라스틱서치의 타입: 데이터베이스의 테이블과 같은 역할
    - 6.0 이하 버전에서는 하나의 인덱스 내부에 기능에 따라 데이터를 분류하고 여러개의 타입을 만들어 사용했지만 현재는 하나의 인덱스에 하나의 타입만을 구성하도록 바뀜
- 엘라스틱서치는 하나의 행을 문서라고 부르며, 해당 문서는 데이터베이스 테이블의 한 행을 의미함
- 엘라스틱서치의 매핑: 필드의 구조와 제약조건에 대한 명세, 관계형 데이터베이스의 스키마
- 관계형 데이터베이스와 엘라스틱서치는 인덱스라는 개념을 다르게 사용함 => 관계형 데이터베이스에서의 인덱스는 WHERE 절의 쿼리와 JOIN을 빠르게 만드는 보조 데이터 도구로 사용됨

- 데이터의 추가, 검색, 수정, 삭제 기능 비교

|엘라스틱서치에서 사용하는 HTTP 메서드|기능|데이터베이스 질의 문법|
|---|---|---|
|GET|데이터 조회|SELECT|
|PUT|데이터 생성|INSERT|
|POST|인덱스 업데이트, 데이터 조회|UPDATE, SELECT|
|DELETE|데이터 삭제|DELETE|
|HEAD|인덱스의 정보 확인|-|

- 엘라스틱서치는 기본적으로 HTTP를 통해 JSON 형식의 RESTful API를 이용
- 엘라스틱서치는 자바로 개발됐지만 여러 가지 프로그래밍 언어를 통해 활용할 수도 있음

- RESTful API는 HTTP 헤더(header)와 URL만 사용해 다양한 형태의 요청을 할 수 있는 HTTP 프로토콜을 최대한 활용하도록 고안된 아키텍처

- 쿼리 비교
    - 데이터베이스에서 SQL문 작성
        - `SELECT * FROM USER WHERE Name like '%가마돈%'`
    - 엘라스틱서치에서 검색엔진이 제공하는 Search API 사용
        - `GET http://localhost:9200/user/_search?q=Name:가마돈`
    - 만약 이름이 영문으로 저장돼 있었다면
        - SQL문은 WHERE 절에 Name like '%Garmadon%'이라고 대소문자를 명확하게 입력해야 검색이 가능하며, 만약 모든 문자열이 소문자이거나 대문자인 경우에는 해당 데이터를 찾을 수 없음
        - 엘라스틱서치는 역색인되는 문자열 전체를 정책에 따라 소문자 혹은 대문자로 생성하고 쿼리가 들어오는 필터를 색인 시간과 검색 시간에 동일하게 지정한다면 해당하는 쿼리에 어떠한 문자열(GARMADON, garmadon, GarMadon)이 들어와도 검색이 가능해짐

- 엘라스틱서치가 관계형 데이터베이스에 비해 강점을 띤 부분 중 하나는 구조화되지 않은 비정형 데이터도 검색이 가능해진다는 점
- 데이터베이스는 스키마를 미리 정의해야만 데이터 저장과 조회가 가능한 반명 엘라스틱서치는 구조화되지 않은 데이터까지 스스로 분석해 자동으로 필드를 생성하고 저장함

## 1.2 검색 시스템과 엘라스틱서치
- 요즘에는 대량의 데이터를 빠르게 검색하기 위해 NoSQL을 많이 사용
- 엘라스틱 서치도 NoSQL의 일종으로서 분류가 가능하고 분산 처리를 통해 실시간에 준하는 빠른 검색이 가능
- 기존 데이터베이스로는 처리하기 어려운 대량의 비정형 데이터도 검색할 수 있으며 전문 검색과 구조 검색 모두를 지원함
- 기본적으로는 검색엔진이지만 MongoDB나 Hbase처럼 대용량 스토리지로도 활용할 수 있음

### 1.2.1 엘라스틱서치가 강력한 이유
- **오픈소스 검색엔진**
    - 엘라스틱서치는 아파치재단의 루씬(Lucene)을 기반으로 개발된 오픈소스 검색엔진
    - 전 세계에서 수많은 사람들이 사용하고 있으며 버그가 발생할 경우에도 대부분 빠르게 해결됨
    - 현재 약 2억 5천만 번 이상 다운로드됨 => 프로젝트가 많이 활성화돼 있음

- **전문 검색**
    - PostgreSQL, MongoDB 같은 대부분의 데이터베이스는 기본 쿼리 및 색인 구조의 한계로 인해 기본적인 텍스트 검색 기능만 제공하지만, 엘라스틱서치는 좀 더 고차원적인 전문 검색(Full Text)이 가능함
    - 전문 검색이란 내용 전체를 색인해서 특정 단어가 포함된 문서를 검색하는 것
    - 기존 관계형 데이터베이스는 전문 검색에 적합하지 않지만 엘라스틱서치는 다양한 기능별, 언어별 플러그인을 조합해 빠르게 검색할 수 있음

- **통계 분석**
    - 비정형 로그 데이터를 수집하고 한곳에 모아 통계 분석을 할 수 있음
    - 엘라스틱서치와 키바나를 연결하면 실시간으로 쌓이는 로그를 시각화하고 분석할 수 있음

- **스키마리스**
    - 데이터베이스는 스키마라는 구조에 따라 데이터를 적합한 형태로 변경해서 저장하고 관리함
    - but 엘라스틱서치는 정형화되지 않은 다양한 형태의 문서도 자동으로 색인하고 검색할 수 있음

- **RESTful API**
    - 엘라스틱서치는 HTTP 기반의 RESTful API를 지원하고 요청뿐 아니라 응답에도 JSON 형식을 사용해 개발 언어, 운영체제, 시스템에 관계없이 이기종 플랫폼에서도 이용 가능

- **멀티테넌시**
    - 서로 상이한 인덱스일지라도 검색할 필드명만 같으면 여러 개의 인덱스를 한번에 조회 가능 => 멀티테넌시 기능 제공

- **Document-Oriented**
    - 여러 계층의 데이터를 JSON 형식의 구조화된 문서로 인덱스에 저장할 수 있음
    - 계층 구조로 문서도 한번의 쿼리로 쉽게 조회할 수 있음

- **역색인**
    - 루씬 기반의 검색엔진이므로 역색인을 지원함
    - MongoDB, 카산드라 같은 일반적인 NoSQL은 역색인을 지원하지 않으므로 다른 NoSQL 대비 엘라스틱서치의 매우 큰 장점임
    - 종이의 마지막 페이지에서 제공하는 색인 페이지와 비슷하게 제공되는 특수한 데이터 구조
    - 역색인된 단어와 문서 번호와의 관계

    |단어|문서 번호|
    |---|---|
    |엘라스틱서치|1|
    |검색엔진|1,2|
    |역색인|2,3|
    |데이터베이스|3|

    - 검색엔진이란 단어가 포함된 모든 문서를 찾아야한다고 할 때, 일반적으로는 처음부터 끝까지 모든 문서를 읽어야만 원하는 결과를 얻을 수 있음
    - but 역색인 구조는 해당 단어만 팢으면 단어가 포함된 모든 문서의 위치를 알 수 있기 때문에 빠르게 찾을 수 있음

- **확장성과 가용성**
    - 10억 개의 문서를 색인한다고 가정하면 모든 문서를 색인하는데 막대한 비용과 시간이 필요하지만, 엘라스틱서치를 분산 구성해서 확장한다면 대량의 문서를 좀 더 효율적으로 처리 가능
    - 분산 환경에서 데이터는 샤드라는 작은 단위로 나뉘어 제공되며, 인덱스를 만들 때마다 샤드의 수를 조절할 수 있 => 데이터의 종류와 성격에 따라 데이터를 분산해서 빠르게 처리할 수 있음

### 1.2.2 엘라스틱서치의 약점
- 실시간이 아님
    - 일반적으로 색인된 데이터는 통상적으로 1초 뒤에나 검색이 가능해짐
    - 색인된 데이터는 내부적으로 커밋과 플러시 같은 복잡한 과정을 거치기 때문에 실시간이 아님
    - 엄밀히 따지자면 준 실시간(Near Realtime)

- 트랜잭션과 롤백 기능 제공하지 않음
    - 엘라스틱서치는 기본적으로 분산 시스템으로 구성됨
    - 전체적인 클러스터의 성능 향상을 위해 시스템적으로 비용 소모가 큰 롤백(Rollback)과 트랜잭션(Transaction)을 지원하지 않기 때문에 최악의 경우 데이터 손실의 위험이 있음

- 데이터의 업데이트를 제공하지 않음
    - 엘라스틱서치는 업데이트 명령이 요청될 경우 기존 문서를 삭제하고 변경된 내용으로 새로운 문서를 생성하는 방식을 사용
    - 단순 업데이트에 비해서는 상대적으로 많은 비용이 발생
    - but 이를 통해 불변적(Immutable)이라는 이점을 취할 수 있기 때문에 큰 단점은 아님

# 3 데이터 모델링
## 3.3 필드 데이터 타입
- 매핑 설정을 위해서는 엘라스틱서치에서 제공하는 데이터 타입으로 어떠한 종류가 있는지 정확하게 이해해야 함
- 이를 바탕으로 데이터의 종류와 형태에 따라 데이터 타입을 선택적으로 사용해야 함
- 데이터 타입
    - `keyword, text 같은 문자열 데이터 타입`
    - `date, long, double, integer, boolean, ip 같은 일반적인 데이터 타입`
    - `객체 또는 중첩문과 같은 JSON 계층의 특성의 데이터 타입`
    - `geo_point, geo_shape 같은 특수한 데이터 타입`

### 3.3.1 Keyword 데이터 타입
- 키워드 형태로 사용할 데이터에 적합한 데이터 타입
- Keyword 데이터 타입을 사용할 경우 별도의 분석기를 거치지 않고 원문 그대로 색인하기 때문에 특정 코드나 키워드 등 정형화된 콘텐츠에 주로 사용됨
- 엘라스틱서치의 일부 기능은 형태소 분석을 하지 않아야만 사용이 가능한데 이 경우에도 Keyword 데이터 타입이 사용됨
- Keyword 데이터 타입이 많이 사용되는 항목
    - `검색 시 필터링 되는 항목`
    - `정렬이 필요한 항목`
    - `집계해야하는 항목`
- 위 3가지 경우에는 반드시 Keyword 타입을 사용해야 함
- 만약 'elastic search'라는 문자열이 Keyword 타입으로 설정되면 'elastic'이나 'search'라는 질의로는 절대 검색되지 않으며 정확하게 'elastic search'라고 질의해야만 검색됨

- Keyword 데이터 타입의의 주요 파라미터
    - boost
        - 필드의 가중치로 검색 결과 정렬에 영향을 줌
        - 기본값은 1.0으로서 1보다 크면 점수가 높게 오르고 적으면 점수가 낮게 오름
        - 이를 이용해 검색에 사용된 키워드와 문서 간의 유사도 스코어 값을 게산할 때 필드의 가중치 값을 얼마나 더 줄 것인지를 판단함
    - doc_values
        - 필드를 메모리에 로드해 캐시로 사용함
        - 기본값은 true
    - index
        - 해당 필드를 검색에 사용할지를 설정
        - 기본값은 true
    - null_value
        - 기본적으로 엘라스틱서치는 데이터의 값이 없으면 필드를 생성하지 않음
        - 데이터의 값이 없는 경우 null로 필드의 값을 대체할지를 설정
    - store
        - 필드 값을 필드와 별도로 _source에 저장하고 검색 가능하게 할지를 설정
        - 기본값은 false

### 3.3.2 Text 데이터 타입
- Text 데이터 타입을 이용하면 색인 시 지정된 분석기가 칼럼의 데이터를 문자열 데이터로 인식하고 이를 분석함
- 만약 별도의 분석기를 정의하지 않았다면 기본적으로 Standard Analyzer를 사용함
- 영화의 제목이나 영화의 설명글과 같이 문장 형태의 데이터에 사용하기 적합한 데이터 타입
- 전문 검색이 가능하다는 점이 가장 큰 특징
- Text 타입으로 데이터를 색인하면 전체 텍스트가 토큰화되어 생성되며 특정 단어를 검색하는 것이 가능해짐
- Text 데이터 타입을 사용할 경우 필드에 검색뿐 아니라 정렬이나 집계 연산을 사용해야할 때가 있음 => 이러한 경우 Text 타입과 Keyword 타입을 동시에 갖도록 멀티 필드로 설정할 수 있음

- Text 데이터 타입의 주요 파라미터
    - analyzer
        - 인덱스와 검색에 사용할 형태소 분석기를 선택
        - 기본값은 Standard Analyzer
    - boost
        - 필드의 가중치로 검색 결과 정렬에 영향을 줌
        - 기본값은 1.0으로 1보다 크면 점수가 높게 오르고 적으면 낮게 오름
    - fielddata
        - 정렬, 집계, 스크립트 등에서 메모리에 저장된 필드 데이터를 사용할지를 설정
        - 기본값은 false
    - index
        - 해당 필드를 검색에 사용할지를 설정
        - 기본값은 true
    - norms
        - 유사도 점수를 산정할 때 필드 길이를 고려할지를 결정
        - 기본값은 true
    - store
        - 필드 값을 필드와 별도로 _source에 저장하고 검색 가능하게 할지를 설정
        - 기본값은 false
    - search_analyzer
        - 검색에 사용할 형태소 분석기를 선택
    - similarity
        - 유사도 점수를 구하는 알고리즘을 선택
        - 기본값은 BM25
    - term_vector
        - Analyzed 필드에 텀벡터를 저장할지를 결정
        - 기본값은 no

### 3.3.3 Array 데이터 타입
- 데이터는 대부분 1차원(하나의 필드에 하나의 값이 매핑)으로 표현되지만 2차원(하나의 필드에 여러 개의 값이 매핑)으로 존재하는 경우도 있음
- 예를 들어, 영화 데이터에 subtitleLang 필드가 있고 해당 필드에는 개봉 영화의 언어 코드 데이터가 들어있다면, 언어의 값으로 영어(en)와 한국어(ko)라는 두 개의 데이터를 입력하고 싶을 경우 Array 데이터 타입을 사용해야 함
- Array 타입은 문자열이나 숫자처럼 일반적인 값을 지정할 수도 있지만 객체 형태로도 정의할 수 있음
- 주의할 점은 Array 타입에 저장되는 값은 모두 같은 타입으로만 구성해야 한다는 것
    - `문자열 배열: ["one", "two"]`
    - `정수 배열: [1, 2]`
    - `객체 배열: [{"name": "Mary", "age": 12}, {"name": "John", "age": 10}]`
- 엘라스틱서치에서는 매핑 설정 시 Array 타입을 명시적으로 정의하지 않음 => 모든 필드가 기본적으로 다수의 값을 가질 수 있기 때문
- 정의된 인덱스 필드에 단순히 배열 값을 입력하면 자동으로 Array 형태로 저장됨
- 만약 필드가 동적으로 추가된다면 배열의 첫 번째 값이 필드의 데이터 타입을 결정하며 이후의 데이터는 모두 같은 타입이어야 색인할 때 오류가 발생하지 않음

### 3.3.4 Numeric 데이터 타입
- 엘라스틱서치에서 숫자 데이터 타입은 여러 가지 종류가 제공됨
- 숫자 데이터 타입이 여러 개 제공되는 이유는 데이터의 크기에 알맞은 타입을 제공함으로써 색인과 검색을 효율적으로 처리하기 위함

- Numeric 데이터 타입
    - long: 최솟값과 최댓값을 가지는 부호 있는 64비트 정수. 범위는 [-2^63 ~ 2^63-1]
    - integer: 최솟값과 최댓값을 가지는 부호 있는 32비트 정수. 범위는 [-2^31 ~ 2^31-1]
    - shor: 최솟값과 최댓값을 가지는 부호 있는 16비트 정수. 범위는 [-32,768 ~ 32,767]
    - byte: 최솟값과 최댓값을 가지는 부호 있는 8비트 정수. 범위는 [-128 ~ 127]
    - double: 64비트 부동소수점을 갖는 수
    - float: 32비트 부동소수점을 갖는 수
    - half_float: 16비트 부동소수점을 갖는 수

### 3.3.5 Date 데이터 타입
- Date 타입은 JSON 포맷에서 문자열로 처리됨
- 날짜는 다양하게 표현될 수 있기 때문에 올바르게 구문 분석될 수 있게 날짜 문자열 형식을 명시적으로 설정해야 함
- 만약 별도의 형식을 지정하지 않을 경우 기본 형식인 "yyyy--MM-ddTHH:mm:ssZ"로 지정됨
- Date 타입은 다음과 같이 크게 세 가지 형태를 제공하는데 세 가지 중 어느 것을 사용해도 내부적으로 UTC의 밀리초 단위로 변환해 저장함
    - `문자열이 포함된 날짜 형식: "2018-04-20", "2018/04.20", "2018-04-20 10:50:00", "2018/04/20 10:50:00"`
    - `ISO_INSTANT 포맷의 날짜 형식: "2018-04-10T10:50:00Z"`
    - `밀리초: 1524449145579`

### 3.3.6 Range 데이터 타입
- 범위가 있는 데이터를 저장할 때 사용
- 숫자뿐 아니라 IP에 대한 범위도 정의 가능

- Range 데이터 타입
    - integer_range: 최솟값과 최댓값을 갖는 부호 있는 32비트 정수 범위
    - float_range: 부동 소수점 값을 갖는 32비트 실수 범위
    - long_range: 최솟값과 최댓값을 갖는 64비트 실수 범위
    - double_range: 부동 소수점 값을 갖는 64비트 실수 범위
    - date_range: 64비트 정수 형태의 밀리초로 표시되는 날짜값의 범위
    - ip_range: IPv4, IPv6 주소를 지원하는 IP 값

### 3.3.7 Boolean 데이터 타입
- 참과 거짓이라는 두 논리값을 가지는 데이터 타입
- 참과 거짓 값을 문자열로 표현하는 것도 가능

- Boolean 데이터 타입
    - 참: true, "true"
    - 거짓: flase, "false"

### 3.3.8 Geo-Point 데이터 타입
- 위도, 경도 등 위치 정보를 담은 데이터를 저장할 때 사용
- 위치 기반 쿼리를 이용해 반경 내 쿼리, 위치 기반 집계, 위치별 정렬 등을 사용할 수 있기 때문에 위치 기반 데이터를 색인하고 검색하는 데 매우 유용함

### 3.3.9 IP 데이터 타입
- IP 주소와 같은 데이터를 저장하는 데 사용
- IPv4나 IPv6를 모두 지정할 수 있음

### 3.3.10 Object 데이터 타입
- JSON 포맷의 문서는 내부 객체를 계층적으로 포함 할 수 있음
- 문서의 필드는 단순히 값을 가질 수도 있지만 복잡한 형태의 또 다른 문서를 포함하는 것도 가능
- 값으로 문서를 가지는 필드의 데이터 타입을 Object 데이터 타입이라고 함
- Object 데이터 타입을 정의할 때는 다른 데이터 타입과 같이 특정 키워드를 이용하지 않고 필드값으로 다른 문서의 구조를 입력함

### 3.3.11Nested 데이터 타입
- Nested 데이터 타입은 Object 객체 배열을 독립적으로 색인하고 질의하는 형태의 데이터 타입
- 특정 필드 내에 Object 형식으로 JSON 포맷을 표현할 수 있으며 필드에 객체가 배열 형태로도 저장될 수 있음
- 데이터가 배열 형태로 저장되면 한 필드 내의 검색은 기본적으로 OR 조건으로 검색됨 => 이러한 특정 탓에 저장되는 데이터의 구조가 조금만 복잡해지면 모호한 상황이 일어날 수 있음 => 이런 문제를 해결하기 위해 nested 데이터 타입이 고안됨 => 검색할 때 일치하는 문서만 정확하게 출력 가능

## 3.4 엘라스틱서치 분석기
### 3.4.1 텍스트 분석 개요
- 엘라스틱서치는 루씬을 기반으로 구축된 텍스트 기반 검색엔진
- 루씬은 내부적으로 다양한 분석기를 제공하는데, 엘라스틱서치는 루씬이 제공하는 분석기를 그대로 활용
- 텍스트 분석을 이해하려면 루씬이 제공하는 분석기가 어떻게 동작하는지를 먼저 이해해야 함

- `"우리나라가 좋은나라, 대한민국 화이팅"`
    - 일반적으로 특정 단어가 포함된 문서를 찾으려면 검색어로 찾을 단어를 입력하면 될 것이라 생각하지만, 엘라스틱서치는 텍스트를 처리하기 위해 기본적으로 분석기를 사용하기 때문에 생각하는 대로 동작하지 않음
    - 예를 들어, 위 문장을 검색하기 위해 "우리나라"라고 입력하면 검색되지 않음 => "우리나라"라는 단어가 존재하지 않기 때문에 해당 문서는 검색되지 않음

- 엘라스틱서치는 문서를 색인하기 전에 해당 문서의 필드 타입이 무엇인지 확인하고 텍스트 타입이면 분석기를 이용해 이를 분석함
- 텍스트가 분석되면 개별 텀으로 나뉘어 형태소 형태로 분석됨
- 해당 형태소는 특정 원칙에 의해 필터링되어 단어가 삭제되거나 추가, 수정되고 최종적으로 역색인됨
- 이러한 방식의 텍스트 분석은 언어별로 조금씩 다르게 동작함
- 이러한 이유로 엘라스틱서치는 각각 다른 언어의 형태소를 분석할 수 있도록 언어별로 분석기를 제공함
- 만약 원하는 언어의 분석기가 없다면 직접 개발하거나 커뮤니티에서 개발한 Custom Analyzer를 설치해서 사용할 수도 있음

- `"우리나라가 좋은나라, 대한민국 화이팅"`
    - Token은 총 4부분으로 나뉨 => "우리나라가", "좋은나라", "대한민국", "화이팅"으로 분리
    - Standard Analyzer를 사용했기 때문에 별도의 형태소 분석은 이뤄지지 않음
    - 어떠한 분석기를 사용하느냐에 따라 분리되는 결과도 달라짐

- 텍스트를 분석할 때 별도의 분석기를 지정하지 않으면 기본적으로 Standard Analyzer가 사용됨

### 3.4.2 역색인 구조
- 어떤 책을 읽을 때 특정한 단어를 알고 있지만 해당 단어가 등장하는 페이지를 알지 못할 때 책의 마지막 부분에 나열된 목록을 보게 됨 => 이 페이지에는 단어와 페이지가 열거돼 있어서 단어가 등장하는 페이지를 펼쳐 내용을 확인할 수 있음
- 이러한 방식으로 특정 단어가 등장하는 페이지를 쉽게 찾아갈 수 있으며 루씬도 이와 비슷하게 동작함
- 루씬의 색인은 역색인이라는 특수한 방식으로 구조화돼 있음

- 역색인 구조
    - `모든 문서가 가지는 단어의 고유 단어 목록`
    - `해당 단어가 어떤 문서에 속해 있는지에 대한 정보`
    - `전체 문서에 각 단어가 몇 개 들어있는지에 대한 정보`
    - `하나의 문서에 단어가 몇 번씩 출현했는지에 대한 빈도`

- 예를 들어, 다음과 같은 텍스트를 가진 2개의 문서가 있음
    - `문서1: elasticsearch is cool`
    - `문서2: Elasticsearch is great`

    - 문서의 역색인을 만들기 위해 각 문서를 토큰화해야 함
    - 토큰화된 단어에 대해 문서 상의 위치와 출현 빈도 등의 정보를 체크함
    
    - 토큰 정보

    |토큰|문서번호|텀의 위치(Position)|텀의 빈도(Term Frequency)|
    |---|---|---|---|
    |elasticsearch|문서1|1|1|
    |Elasticsearch|문서2|1|1|
    |is|문서1,문서2|2,2|2|
    |cool|문서1|3|1|
    |great|문서2|3|1|

    - 위 내용을 살펴보면 토큰이 문서의 어디에 위치하고 몇 번 나왔는지(빈도)에 대한 정보를 알 수 있음
    - 이 표를 참고하면 검색어가 존재하는 문서를 찾기 위해 검색어와 동일한 토큰을 찾아 해당 토큰이 존재하는 문서 번호를 찾아가면 됨
    - 만약 "cool"을 검색어로 지정하면 문서1의 내용이 검색 결과로 나옴

    - "elasticsearch"를 검색어로 지정하면 문서1과 문서2에 해당하는 내용이 다 나와야 할 것이라고 예상하겠지만 토큰의 정보가 정확하게 일치하는 데이터만 출력하기 때문에 문서1만 출력되고 문서2는 출력되지 않음
    - 이 문제를 해결하기 위한 간단한 방법은 색인 전에 텍스트 전체를 소문자로 변환한 다음 색인하는 것 => 그러면 두 개의 문서가 "elasticsearch"라는 토큰으로 나옴
    
    - 변경된 토큰 정보

    |토큰|문서번호|텀의 위치(Position)|텀의 빈도(Term Frequency)|
    |---|---|---|---|
    |elasticsearch|문서1,문서2|1,1|2|
    |is|문서1,문서2|2,2|2|
    |cool|문서1|3|1|
    |great|문서2|3|1|

    - 색인한다는 것은 역색인 파일을 만든다는 것
    - 그렇다고 원문 자체를 변경한다는 의미는 아님
    - 따라서 색인 파일에 들어갈 토큰만 변경되어 저장되고 실제 문서의 내용은 변함없이 저장됨
    - 색인할 때 특정한 규칙과 흐름에 의해 텍스트를 변경하는 과정을 분석(Analyze)이라고 하고 해당 처리는 분석기(Analyzer)라는 모듈을 조합해서 이뤄짐

### 3.4.3 분석기의 구조
- 분석기는 기본적으로 다음과 같은 프로세스로 동작함
    1. 문장을 특정한 규칙에 의해 수정한다.
    2. 수정한 문장을 개별 토큰으로 분리한다.
    3. 개별 토큰을 특정한 규칙에 의해 변경한다.

- 이 세 가지 동작은 특성에 의해 각각 다음과 같은 용어로 불림

    - **CHARACTER FILTER**
        - 문장을 분석하기 전에 입력 텍스트에 대해 특정한 단어를 변경하거나 HTML과 같은 태그를 제거하는 역할을 하는 필터
        - 해당 내용은 텍스트를 개별 토큰화하기 전의 전처리 과정이며, ReplaceAll() 함수처럼 패턴으로 텍스트를 변경하거나 사용자가 정의한 필터를 적용할 수 있음

    
    - **TOKENIZER FILTER**
        - Tokenizer Filter는 분석기를 구성할 때 하나만 사용할 수 있으며 텍스트를 어떻게 나눌 것인지를 정의함
        - 한글을 분해할 때는 한글 형태소 분석기의 Tokenizer를 사용하고, 영문을 분석할 때는 영문 형태소 분석기의 Tokenizer를 사용하는 등 상황에 맞게 적절한 Tokenizer를 사용하면 됨

    - **TOKEN FILTER**
        - 토큰화된 단어를 하나씩 필터링해서 사용자가 원하는 토큰으로 변환함
        - 예를 들어, 불필요한 단어를 제거하거나 동의어 사전을 만들어 단어를 추가하거나 영문 단어를 소문자로 변환하는 등의 작업을 수행할 수 있음
        - Token Filter는 여러 단계가 순차적으로 이뤄지며 순서를 어떻게 지정하느냐에 따라 검색의 질이 달라질 수 있음

- 형태소 분석 프로세스
    - 문장 => `Character Filter` => 가공된 문장 => `Tokenizer Filter` => Terms => `Token Filter` <=> `사전` => 변경된 Terms => `index`

- 분석기는 데이터의 특성에 따라 원하는 분석 결과를 미리 예상해보고 해당 결과를 얻기 위한 옵션을 적용해 설정해야 함
- 분석 결과를 미리 예상할 수 있다는 것은 내부적인 동작 과정을 모두 이해하고 있다는 의미
- 이러한 과정은 많은 경험이 필요하기 때문에 다양한 경우를 대상으로 직접 테스트해 보고 특성을 파악해야 함

- `<B>Elasticsearch</B> is cool`
    - 이 문서에서 불필요한 HTML 태그를 제거하고 문장의 대문자를 모두 소문자로 변형해서 인덱스를 저장하는 프로세스를 구성해야 함
    - `custom_movie_analyzer`
        - `char_filter: html_strip`: Character Filter를 정의, 전체 텍스트 문장에서 HTML태그를 제거함
        - `tokenizer: standard`: Tokenizer Filter를 정의, 특수문자 혹은 공백을 기준으로 텍스트를 분할
        - `filter: lowercase`: Token Filter를 정의, 모든 토큰을 소문자로 변환

    - 토큰 정제 흐름
        1. `<B>Elasticsearch</B> is cool` => Character Filter: "html_strip"에 의해 HTML 제거
        2. `Elasticsearch is cool` => Tokenizer: "Standard" 토크나이저로 Term 분리
        3. `Elasticsearch -> Token1, Position1`, `is -> Token2, Position2`, `cool -> Token3, Position3` => Token Filter: "lowercase" 필터로 소문자 처리
        4. `elasticsearch -> Token1, Position1`, `is -> Token2, Position2`, `cool -> Token3, Position3`

#### 3.4.3.1 분석기 사용법
- 엘라스틱서치는 루씬에 존재하는 기본 분석기를 별도의 정의 없이 사용할 수 있게 미리 정의해서 제공함
- "standard"라는 키워드는 루씬의 Standard Analyzer를 의미하며 이러한 분석기를 사용하기 위해 엘라스틱서치에서는 _analyze API를 제공함

- **분석기를 이용한 분석**
    - 엘라스틱서치에서는 형태소가 어떻게 분석되는지를 확인할 수 있는 _analyze API를 제공함
    - 미리 정의된 분석기의 경우 이를 이용해 쉽게 테스트해볼 수 있음

- **필드를 이용한 분석**
    - 인덱스를 설정할 때 분석기를 직접 설정할 수 있음
    - 이때 다양한 옵션과 필터를 적용해 분석기를 설정할 수 있는데 이렇게 설정한 분석기를 매핑 설정을 통해 칼럼에 지정할 수 있음
    
- **색인과 검색 시 분석기를 각각 설정**
    - 분석기는 색인할 때 사용되는 Index Analyzer와 검색할 때 사용되는 Search Analyzer로 구분해서 구성할 수도 있음
    - 인덱스를 생성할 때 색인용과 검색용 분석기를 각각 정의하고 적용하고자 하는 필드에 원하는 분석기를 지정하면 됨

### 3.4.3.2 대표적인 분석기
- 엘라스틱서치에서는 루씬에 존재하는 대부분의 분석기를 기본 분석기로 제공함

- **Stnadard Analyzer**
    - 인덱스를 생성할 때 settings에 analyzer를 정의하게 됨
    - 하지만 아무런 정의를 하지 않고 필드의 데이터 타입을 Text 데이터 타입으로 사용한다면 기본적으로 Standard Analyzer를 사용함
    - 이 분석기는 공백 혹은 특수기호를 기준으로 토큰을 분리하고 모든 문자를 소문자로 변경하는 토큰 필터를 사용함
    - Stnadard Analyzer의 구성 요소
        - Tokenizer: Standard Tokenizer
        - Token Filter: Standard Token Filter, Lower Case Token Filter
    - Standard Analyzer 옵션
        - 파라미터
            - max_token_length: 최대 토큰 길이를 초과하는 토큰이 보일 경우 해당 length 간격으로 분할함, 기본값은 255자
            - stopwords: 사전 정의된 불용어 사전을 사용함, 기본값은 사용하지 않음
            - stopwords_path: 불용어가 포함된 파일을 사용할 경우의 서버의 경로로 사용
    
- **Whitespace 분석기**
    - 공백 문자열을 기준으로 토큰을 분리하는 간단한 분석기
    - Whitespace 분석기의 구성 요소
        - Tokenizer: Whitespace Tokenizer
        - Token Filter: 없음

- **Keyword 분석기**
    - 전체 입력 문자열을 하나의 키워드처럼 처리
    - 토큰화 작업을 하지 않음
    - 분석기의 구성 요소
        - Tokenizer: Keyword Tokenizer
        - Token Filter: 없음

### 3.4.4 전처리 필터
- 엘라스틱서치에서 제공하는 분석기는 전처리 필터(Characteeer Filter)를 이용한 데이터 정제 후 토크나이저를 이용해 본격적인 토큰 분리 작업을 수행함
- 그런 다음, 생성된 토큰 리스트를 토큰 필터를 통해 재가공하는 3단계 방식으로 동작함
- 하지만 토크나이저 내부에서도 일종의 전처리가 가능하기 때문에 전처리 필터는 상대적으로 활용도가 많이 떨어짐

- **Html strip char 필터**
    - 문장에서 HTML을 제거하는 전처리 필터
    - Html strip char 필터 옵션
        - 파라미터
            - escaped_tags: 특정 태그만 삭제함, 기본값으로 HTML 태그를 전부 삭제함

### 3.4.5 토크나이저 필터
- 토크나이저 필터는 분석기를 구성하는 가장 핵심 구성요소
- 전처리 필터를 거쳐 토크나이저 필터로 문서가 넘어오면 해당 텍스트는 Tokenizer의 특성에 맞게 적절히 분해됨
- 분석기에서 어떠한 토크나이저를 사용하느냐에 따라 분석기의 전체적인 성격이 결정됨

- **Standard 토크나이저**
    - 엘라스틱서치에서 일반적으로 사용하는 토크나이저로서 대부분의 기호를 만나면 토큰으로 나눔
    - Standard 토크나이저 옵션
        - 파라미터
            - max_token_length: 최대 토큰 길이를 초과하는 경우 해당 간격으로 토큰을 분할함, 기본값은 255

- **WHITESPACE TOKENIZER**
    - 공백을 만나면 텍스트를 토큰화함
    - Whitespace 토크나이저 옵션
        - 파라미터
            - max_token_length: 최대 토큰 길이를 초과하는 경우 해당 간격으로 토큰을 분할함, 기본값은 255

- **Ngram 토크나이저**
    - Ngram은 기본적으로 한 글자씩 토큰화함
    - Ngram에 특정 문자를 지정할 수도 있으며, 이 경우 지정된 문자의 목록 중 하나를 만날 때마다 단어를 자름
    - 그 밖에도 다양한 옵션을 조합해서 자동완성을 만들 때 유용하게 활용할 수 있음
    - Ngram 토크나이저 옵션
        - 파라미터
            - min_gram: Ngram을 적용할 문자의 최소 길이, 기본값은 1
            - max_gram: Ngram을 적용할 문자의 최대 길이, 기본값은 2
            - token_chars: 토큰에 포함할 문자열을 지정, 다양한 옵션 제공
                - letter(문자)
                - digit(숫자)
                - whitespace(공백)
                - punctuation(구두점)
                - symbol(특수기호)

- **Edge Ngram 토크나이저**
    - 지정된 문자의 목록 중 하나를 만날 때마다 시작 부분을 고정시켜 단어를 자르는 방식으로 사용하는 토크나이저
    - 자동완성을 구현할 때 유용함
    - Edge Ngram 토크나이저 옵션
        - 파라미터
            - min_gram: Ngram을 적용할 문자의 최소 길이, 기본값은 1
            - max_gram: Ngram을 적용할 문자의 최대 길이, 기본값은 2
            - token_chars: 토큰에 포함할 문자열을 지정, 다양한 옵션 제공
                - letter(문자)
                - digit(숫자)
                - whitespace(공백)
                - punctuation(구두점)
                - symbol(특수기호)

- **Keyword 토크나이저**
    - 텍스트를 하나의 토큰으로 만듦
    - Keyword 토크나이저 옵션
        - 파라미터
            - buffer_size: 텀을 버퍼로 읽어 들일 문자 수를 지정함, 기본값은 256

### 3.4.6 토큰 필터
- 토큰 필터(Token Filter)는 토크나이저에서 분리된 토큰들을 변형하거나 추가, 삭제할 때 사용하는 필터
- 토크나이저에 의해 토큰이 모두 분리되면 분리된 토큰은 배열 형태로  토큰 필터로 전달됨
- 토크나이저에 의해 토큰이 모두 분리돼야 비로소 동작하기 대문에 독립적으로는 사용 불가능

- **Ascii Folding 토큰 필터**
    - 아스키 코드에 해당하는 127개의 알파벳, 숫자, 기호에 해당하지 않는 경우 문자를 ASCII 요소로 변경함

- **Lowercase 토큰 필터**
    - 토큰을 구성하는 전체 문자열을 소문자로 변환함

- **Uppercase 토큰 필터**
    - Lowercase 토큰 필터와는 반대로 전체 문자열을 대문자로 변환함

- **Stop 토큰 필터**
    - 불용어로 등록할 사전을 구축해서 사용하는 필터
    - 인덱스로 만들고 싶지 않거나 검색되지 않게 하고 싶은 단어를 등록해서 해당 단어에 대한 불용어 사전을 구축함
    - Stop 토큰 필터 옵션
        - 파라미터
            - stopwords: 불용어를 매핑에 직접 등록해서 사용
            - stopwords_path: 불용어 사전이 존재하는 경로를 지정, 해당 경로는 엘라스틱서치 서버가 있는 config 폴더 안에 생성
            - ingnore_case: true로 지정할 경우 모든 단어를 소문자로 변경해서 저장, 기본값은 false
        
- **Stemmer 토큰 필터**
    - Stemming 알고리즘을 사용해 토큰을 변형하는 필터
    - Stemmer 토큰 필터 옵션
        - 파라미터
            - name: english, light_english, minimal_english, possessive_english, porter2, lovins 등 다른 나라의 언어도 사용 가능하지만 한글은 지원하지 않음

- **Synonym 토큰 필터**
    - 동의어를 처리할 수 있는 필터
    - Synonym 토큰 필터 옵션
        - 파라미터
            - synonyms: 동의어로 사용할 단어를 등록
            - synonyms_path: 파일로 관리할 경우 엘라스틱서치 서버의 config 폴더 아래에 생성

- **Trim 토큰 필터**
    - 앞뒤 공백을 제거하는 토큰 필터

### 3.4.7 동의어 사전
- 토크나이저에 의해 토큰이 모두 분리되면 다양한 토큰 필터를 적용해 토큰을 가공할 수 있음
- 토큰 필터를 이용하면 토큰을 변경하는 것은 물론이고 토큰을 추가하거나 삭제하는 것도 가능
- 엘라스틱서치에서 제공하는 토큰 필터 중 Synonym 필터를 사용하면 동의어 처리가 가능해짐
- 동의어는 검색 기능을 풍부하게 할 수 있게 도와주는 도구 중 하나
- 원문에 특정 단어가 존재하지 않더라도 색인 데이터를 토큰화해서 저장할 때 동의어나 유의어에 해당하는 단어를 함께 저장해서 검색이 가능해지게 하는 기술
- 동의어를 추가하는 방식
    - 동의어를 매핑 설정 정보에 미리 파라미터로 등록
    - 특정 파일을 별도로 생성해서 관리
- 엘라스틱서치에서 가장 까다로운 부분 중 하나가 바로 동의어를 관리하는 것
- 검색엔진에서 다루는 분야가 많아지면 많아질수록 동의어의 수도 늘어남
- 분야별로 파일도 늘어날 것이고 그 안의 동의어 변환 규칙도 많아질 것
- 실무에서는 이러한 동의어를 모아둔 파일들을 칭할 때 일반적으로 "동의어 사전"이라는 용어를 사용함

- **동의어 사전 만들기**
    - 동의어 파일은 엘라스틱서치가 설치된 서버 아래의 config 디렉터리에 생성해야 함

- **동의어 추가**
    - 동의어를 추가할 때 단어를 쉼표(,)로 분리해 등록하는 방법

- **동의어 치환하기**
    - 특정 단어를 어떤 단어로 변경하고 싶다면 동의어 치환 기능 이용
    - 동의어를 치환하면 원본 토큰이 제거되고 변경될 새로운 토큰이 추가됨
    - 동의어 치환은 동의어 추가와 구분하기 위해 화살표(=>)로 표시함

- 동의어 사전은 실시간으로 적용되지 않음 => 수정된 동의어를 적용하고 싶다면 해당 동의어 사전을 사용하고 있는 인덱스를 Reload해야 함
- 주의할 점
    - 동의어 사전은 색인 시점에도 사용될 수 있고 검색 시점에도 사용될 수 있음
    - 검색 시점에는 사전의 내용이 변경되더라도 해당 내용이 반영되지만, 색인 시점에 동의어 사전이 사용됐다면 사전의 내용이 변경되더라도 색인이 변경되지 않음 => 이 경우에는 기존 색인을 모두 삭제하고 색인을 다시 생성해야만 변경된 사전의 내용이 적용됨 => 이러한 문제점 때문에 동의어 사전이 빈번하게 수정되는 인덱스의 경우 색인 시점에는 적용하지 않고 검색 시점에만 적용하는 방식으로 문제를 해결하기도 함